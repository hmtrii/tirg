{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Compute_recall.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hmtrii/tirg/blob/main/Compute_recall.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPTV_2MvyvWG",
        "outputId": "3e9b4fce-3843-4c8f-ec39-8afa0e02c73c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLwLuW2_dWRp"
      },
      "source": [
        "# INSTALL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AfCDwglHa42",
        "outputId": "9ce17472-5daa-4707-e764-4c74486721ad"
      },
      "source": [
        "!git clone https://github.com/hmtrii/tirg.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'tirg'...\n",
            "remote: Enumerating objects: 103, done.\u001b[K\n",
            "remote: Counting objects: 100% (103/103), done.\u001b[K\n",
            "remote: Compressing objects: 100% (72/72), done.\u001b[K\n",
            "remote: Total 103 (delta 55), reused 71 (delta 29), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (103/103), 3.30 MiB | 17.14 MiB/s, done.\n",
            "Resolving deltas: 100% (55/55), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFtMdH2JL0t5"
      },
      "source": [
        "!rm -rf /content/Fashion200k\n",
        "!mkdir /content/Fashion200k\n",
        "!cp -r /content/drive/MyDrive/TIRG/dataset/fashion-200k/labels /content/Fashion200k\n",
        "!cp /content/drive/MyDrive/TIRG/dataset/test_queries.txt /content/Fashion200k\n",
        "!mkdir /content/Fashion200k/women"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udvOUNWT3JxZ"
      },
      "source": [
        "!tar -xf /content/drive/MyDrive/TIRG/dataset/fashion-200k/women.tar.gz -C /content/Fashion200k/women"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9b8NIwraH8L",
        "outputId": "32df495a-11dc-4168-e7f2-7fae7470efe9"
      },
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\r\u001b[K     |█                               | 10kB 20.6MB/s eta 0:00:01\r\u001b[K     |██▏                             | 20kB 14.9MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30kB 13.1MB/s eta 0:00:01\r\u001b[K     |████▎                           | 40kB 12.4MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 61kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 81kB 9.7MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 92kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 102kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 112kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 122kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 133kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 143kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████                | 153kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 163kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 174kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 184kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 194kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 204kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 215kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 225kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 235kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 245kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 256kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 266kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 276kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 286kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 296kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 307kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 317kB 8.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.19.4)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (50.3.2)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RarrJevA66j8",
        "outputId": "e88a5c69-af5b-4c4a-9e4f-408b8d5f12d3"
      },
      "source": [
        "!pip install torch==1.2.0 torchvision==0.4.0    "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/57/d5cceb0799c06733eefce80c395459f28970ebb9e896846ce96ab579a3f1/torch-1.2.0-cp36-cp36m-manylinux1_x86_64.whl (748.8MB)\n",
            "\u001b[K     |████████████████████████████████| 748.9MB 18kB/s \n",
            "\u001b[?25hCollecting torchvision==0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/e6/a564eba563f7ff53aa7318ff6aaa5bd8385cbda39ed55ba471e95af27d19/torchvision-0.4.0-cp36-cp36m-manylinux1_x86_64.whl (8.8MB)\n",
            "\u001b[K     |████████████████████████████████| 8.8MB 10.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.2.0) (1.19.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4.0) (1.15.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4.0) (7.0.0)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.7.0+cu101\n",
            "    Uninstalling torch-1.7.0+cu101:\n",
            "      Successfully uninstalled torch-1.7.0+cu101\n",
            "  Found existing installation: torchvision 0.8.1+cu101\n",
            "    Uninstalling torchvision-0.8.1+cu101:\n",
            "      Successfully uninstalled torchvision-0.8.1+cu101\n",
            "Successfully installed torch-1.2.0 torchvision-0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "NlgkKV1lK591",
        "outputId": "73dfef52-ff4f-4c27-dc8b-1c2eb1b138b2"
      },
      "source": [
        "!pip install Pillow==5.2.0"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Pillow==5.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/24/f53ff6b61b3d728b90934bddb4f03f8ab584a7f49299bf3bde56e2952612/Pillow-5.2.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 9.0MB/s \n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Pillow\n",
            "  Found existing installation: Pillow 7.0.0\n",
            "    Uninstalling Pillow-7.0.0:\n",
            "      Successfully uninstalled Pillow-7.0.0\n",
            "Successfully installed Pillow-5.2.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZshH0u2dbWb"
      },
      "source": [
        "# LOAD DATASET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVAi4Q6adUeR",
        "outputId": "a70788e8-183a-4ce4-c0df-328a4cbce3c0"
      },
      "source": [
        "cd /content/tirg"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/tirg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNn9zPOksjqu"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "import numpy as np\n",
        "from tools import pkl\n",
        "import time\n",
        "from test_retrieval import test\n",
        "from main import load_dataset, create_model_and_optimizer\n",
        "from LSH import create_hash_table"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OD8iosGbdh1O"
      },
      "source": [
        "class Opt:\n",
        "    def __init__(self):\n",
        "        self.dataset = \"fashion200k\"\n",
        "        self.dataset_path = \"/content/Fashion200k\"\n",
        "        self.batch_size = 32\n",
        "        self.embed_dim = 512\n",
        "        self.hashing = True\n",
        "        self.retrieve_by_random = True\n",
        "        self.model = \"tirg\"\n",
        "        self.learning_rate = 1e-2\n",
        "        self.weight_decay = 1e-6\n",
        "        self.pretrained_model = \"/content/drive/MyDrive/TIRG/runs/exp2/latest_checkpoint.pth\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdB0YklCdkpC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2b0506d-7f6b-4c24-88bb-18266df66993"
      },
      "source": [
        "opt = Opt()\n",
        "trainset, testset = load_dataset(opt)\n",
        "model, _ = create_model_and_optimizer(opt, [t for t in trainset.get_all_texts()])\n",
        "model.eval()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading dataset  fashion200k\n",
            "read dress_train_detect_all.txt\n",
            "read jacket_train_detect_all.txt\n",
            "read pants_train_detect_all.txt\n",
            "read top_train_detect_all.txt\n",
            "read skirt_train_detect_all.txt\n",
            "Fashion200k: 172049 images\n",
            "53099 unique captions\n",
            "Modifiable images 106464\n",
            "read top_test_detect_all.txt\n",
            "read jacket_test_detect_all.txt\n",
            "read skirt_test_detect_all.txt\n",
            "read dress_test_detect_all.txt\n",
            "read pants_test_detect_all.txt\n",
            "Fashion200k: 29789 images\n",
            "trainset size: 172049\n",
            "testset size: 29789\n",
            "Creating model and optimizer for tirg\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/checkpoints/resnet18-5c106cde.pth\n",
            "100%|██████████| 44.7M/44.7M [00:07<00:00, 6.15MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Load checkpoint from /content/drive/MyDrive/TIRG/runs/exp2/latest_checkpoint.pth\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TIRG(\n",
              "  (normalization_layer): NormalizationLayer()\n",
              "  (soft_triplet_loss): TripletLoss()\n",
              "  (img_model): ResNet(\n",
              "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (layer1): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer4): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): GlobalAvgPool2d()\n",
              "    (fc): Sequential(\n",
              "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (text_model): TextLSTMModel(\n",
              "    (embedding_layer): Embedding(5590, 512)\n",
              "    (lstm): LSTM(512, 512)\n",
              "    (fc_output): Sequential(\n",
              "      (0): Dropout(p=0.1, inplace=False)\n",
              "      (1): Linear(in_features=512, out_features=512, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (gated_feature_composer): Sequential(\n",
              "    (0): ConCatModule()\n",
              "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
              "  )\n",
              "  (res_info_composer): Sequential(\n",
              "    (0): ConCatModule()\n",
              "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    (4): ReLU()\n",
              "    (5): Linear(in_features=1024, out_features=512, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3S-Bs8H-jVI5"
      },
      "source": [
        "Compute all images feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mS7tC5-hxbv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b24217ed-30d8-4e5e-ef03-09c76b2ce1de"
      },
      "source": [
        "# all_imgs = []\n",
        "# imgs = []\n",
        "# for i in tqdm(range(len(testset.imgs))):\n",
        "#     imgs += [testset.get_img(i)]\n",
        "#     if len(imgs) >= opt.batch_size or i == len(testset.imgs) - 1:\n",
        "#         if 'torch' not in str(type(imgs[0])):\n",
        "#             imgs = [torch.from_numpy(d).float() for d in imgs]\n",
        "#         imgs = torch.stack(imgs).float()\n",
        "\n",
        "#         if torch.cuda.is_available():\n",
        "#             imgs = torch.autograd.Variable(imgs).cuda()\n",
        "#         else:\n",
        "#             imgs = torch.autograd.Variable(imgs).cpu()\n",
        "\n",
        "#         imgs = model.extract_img_feature(imgs).data.cpu().numpy()\n",
        "#         all_imgs += [imgs]\n",
        "#         imgs = []\n",
        "# all_imgs = np.concatenate(all_imgs)\n",
        "# all_captions = [img['captions'][0] for img in testset.imgs]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 29789/29789 [04:36<00:00, 107.54it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekJvwwFyjzwN"
      },
      "source": [
        "Compute all queries feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuWzN0pBj5Or",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f430ffbe-9a69-45c9-809c-72db62e64271"
      },
      "source": [
        "# imgs = []\n",
        "# mods = []\n",
        "# all_queries = []\n",
        "# test_queries = testset.get_test_queries()\n",
        "# for t in tqdm(test_queries):\n",
        "#     imgs += [testset.get_img(t['source_img_id'])]\n",
        "#     mods += [t['mod']['str']]\n",
        "#     if len(imgs) >= opt.batch_size or t is test_queries[-1]:\n",
        "#         if 'torch' not in str(type(imgs[0])):\n",
        "#             imgs = [torch.from_numpy(d).float() for d in imgs]\n",
        "#         imgs = torch.stack(imgs).float()\n",
        "\n",
        "#         if torch.cuda.is_available():\n",
        "#             imgs = torch.autograd.Variable(imgs).cuda()\n",
        "#         else:\n",
        "#             imgs = torch.autograd.Variable(imgs).cpu()\n",
        "\n",
        "#         mods = [t for t in mods]\n",
        "#         f = model.compose_img_text(imgs, mods).data.cpu().numpy()\n",
        "#         all_queries += [f]\n",
        "#         imgs = []\n",
        "#         mods = []\n",
        "# all_queries = np.concatenate(all_queries)\n",
        "# all_target_captions = [t['target_caption'] for t in test_queries]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 17/33480 [00:00<03:19, 167.58it/s]/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1350: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "100%|██████████| 33480/33480 [04:20<00:00, 128.45it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7Q64sjm0yy7"
      },
      "source": [
        "test_queries = testset.get_test_queries()\n",
        "all_queries = pkl.pkl_load(\"/content/drive/MyDrive/TIRG/pkl_ver2/all_norm_queries_feature.pkl\")\n",
        "all_target_captions = pkl.pkl_load(\"/content/drive/MyDrive/TIRG/pkl_ver2/all_target_captions.pkl\")\n",
        "all_imgs = pkl.pkl_load(\"/content/drive/MyDrive/TIRG/pkl_ver2/all_norm_imgs_feature.pkl\")\n",
        "all_captions = pkl.pkl_load(\"/content/drive/MyDrive/TIRG/pkl_ver2/all_captions.pkl\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIQagWaKmirM"
      },
      "source": [
        "# Create random queries\n",
        "np.random.seed(4)\n",
        "id_random = np.random.choice(all_queries.shape[0], 5000)\n",
        "queries_feature = []\n",
        "for id in id_random:\n",
        "    queries_feature += [all_queries[id]]\n",
        "\n",
        "queries_feature = np.array(queries_feature)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08HVkeSij9ay"
      },
      "source": [
        "# Normal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80vLcayhfA5z",
        "outputId": "c39f4bb7-8aaa-449a-bfe4-ad0376c8af19"
      },
      "source": [
        "tic = time.time()\n",
        "sims = queries_feature.dot(all_imgs.T)\n",
        "for i, t in enumerate(test_queries):\n",
        "\ttry:\n",
        "\t\tsims[i, t[\"source_img_id\"]] = -10e10  # remove query image\n",
        "\texcept:\n",
        "\t\tpass\n",
        "\n",
        "nn_result = [np.argsort(-sims[i, :])[:110] for i in range(sims.shape[0])]\n",
        "out = []\n",
        "nn_result = [[all_captions[nn] for nn in nns] for nns in nn_result]\n",
        "# for k in [1, 5, 10, 50, 100]:\n",
        "r = 0.0\n",
        "# for i, nns in enumerate(nn_result):\n",
        "for id, nns in zip(id_random, nn_result):\n",
        "    if all_target_captions[id] in nns[:100]:\n",
        "        r += 1\n",
        "r /= len(nn_result)\n",
        "out += [('recall_top' + str(k) + '_correct_composition', 100)]\n",
        "tac = time.time()\n",
        "print(out)\n",
        "print(f\"{round(tac - tic, 3)}s\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('recall_top100_correct_composition', 100)]\n",
            "13.362s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd8cl_aSqIhX"
      },
      "source": [
        "# Hashing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HtxJRBtqOeG"
      },
      "source": [
        "def compute_similary_by_hasing(all_imgs_feature, queries_feature, test_queries):\n",
        "\thash_table = create_hash_table(all_imgs_feature, 12, 512)\n",
        "\tsims = []\n",
        "\tfor i, query in enumerate(queries_feature):\n",
        "\t\tidx_imgs = hash_table.__getitem__(query)\n",
        "\t\tspatial_search = np.array([all_imgs_feature[i] for i in idx_imgs])\n",
        "\t\tif spatial_search.shape[0] != 0:\n",
        "\t\t\tdis_value = query.dot(spatial_search.T)\n",
        "\t\t\tsim = np.full(all_imgs_feature.shape[0], -10e10)\n",
        "\t\t\tfor i in idx_imgs:\n",
        "\t\t\t\tsim[i] = dis_value[0]\n",
        "\t\t\t\tdis_value = dis_value[1:]\n",
        "\t\telse:\n",
        "\t\t\tsim = query.dot(all_imgs_feature.T)\n",
        "\n",
        "\t\tsim[test_queries[i]['source_img_id']] = -10e10\n",
        "\t\tsims += [sim]\n",
        "\n",
        "\treturn np.array(sims)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSgb4u1eqSno",
        "outputId": "cb7912d1-46a4-4c0b-b44d-2b7a6fda19c0"
      },
      "source": [
        "tic = time.time()\n",
        "sims = compute_similary_by_hasing(all_imgs, queries_feature, test_queries)\n",
        "nn_result = [np.argsort(-sims[i, :])[:100] for i in range(sims.shape[0])]\n",
        "out = []\n",
        "nn_result = [[all_captions[nn] for nn in nns] for nns in nn_result]\n",
        "# for k in [1, 5, 10, 50, 100]:\n",
        "r = 0.0\n",
        "# for i, nns in enumerate(nn_result):\n",
        "for id, nns in zip(id_random, nn_result):\n",
        "    if all_target_captions[id] in nns[:100]:\n",
        "        r += 1\n",
        "r /= len(nn_result)\n",
        "out += [('recall_top' + str(100) + '_correct_composition', r)]\n",
        "tac = time.time()\n",
        "print(out)\n",
        "print(f\"{round(tac - tic, 3)}s\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('recall_top100_correct_composition', 0.2656)]\n",
            "11.093s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}